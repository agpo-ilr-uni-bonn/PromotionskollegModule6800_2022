{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71QCGinKTAAf"
      },
      "source": [
        "# Day 1 : Code used during lecture and lab assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkq_ugJnc0qD"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "- The notebook combines 'code used during lecture' with the 'Day 1 lab' assignment (see further down)\n",
        "- The lab assignment can be done largely by copying/paste/modification of the code used during the lecture\n",
        "- Please add answers/discussion/comments to the notebook as comments or text box. Do not create another file in addition.\n",
        "- When you are done with your assignment, save the notebook in drive and add your last name to the name of the file\n",
        "- Upload your final notebook to https://uni-bonn.sciebo.de/s/PRBpXWkIandOj0M latest by September 30th. The password for access was sent to you by email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBjm-4D-Tg-L"
      },
      "source": [
        "# Code used during lecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xn9LO_jCSar8"
      },
      "outputs": [],
      "source": [
        "# Import libaries that will be used in the notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0aKGu2ArXhh",
        "outputId": "52f5326b-401b-462c-fa4a-907c7206cc41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-19 12:40:31--  https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz\n",
            "Resolving ilr-ml.s3.eu-central-1.amazonaws.com (ilr-ml.s3.eu-central-1.amazonaws.com)... 52.219.170.86\n",
            "Connecting to ilr-ml.s3.eu-central-1.amazonaws.com (ilr-ml.s3.eu-central-1.amazonaws.com)|52.219.170.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283350352 (270M) [application/x-gzip]\n",
            "Saving to: ‘brazil_all_data_v2.gz’\n",
            "\n",
            "brazil_all_data_v2. 100%[===================>] 270.22M  28.3MB/s    in 10s     \n",
            "\n",
            "2022-09-19 12:40:42 (26.0 MB/s) - ‘brazil_all_data_v2.gz’ saved [283350352/283350352]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download data\n",
        "!wget -nc https://ilr-ml.s3.eu-central-1.amazonaws.com/brazil_all_data_v2.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mystC39Lldkv"
      },
      "outputs": [],
      "source": [
        "# Load data with pandas into a dataframe \n",
        "df = pd.read_parquet('brazil_all_data_v2.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "snUuhtBAmtnW",
        "outputId": "97aa3f56-6239-4e9c-e9ca-1de3467425cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 249940\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  row  col        lon        lat       bean     carrot  cassava  \\\n",
              "0   0    0    0 -59.989876 -10.010125  200.00000  335.00000    201.0   \n",
              "1   1    0    1 -59.969875 -10.010125  200.00000  335.00000    201.0   \n",
              "2   2    0    2 -59.949875 -10.010125  200.00000  335.00000    201.0   \n",
              "3   3    0    3 -59.929874 -10.010125  200.00000  335.00000    201.0   \n",
              "4   4    0    4 -59.909874 -10.010125  218.33334  435.83334    216.0   \n",
              "\n",
              "   chickpea  citrus  ...  tot_defor_2010_lag_3rd_order  \\\n",
              "0       0.0   391.0  ...                      1.800000   \n",
              "1       0.0   391.0  ...                      1.052631   \n",
              "2       0.0   391.0  ...                      3.652174   \n",
              "3       0.0   391.0  ...                      3.814815   \n",
              "4       0.0   523.5  ...                      8.296296   \n",
              "\n",
              "   tot_defor_2011_lag_3rd_order  tot_defor_2012_lag_3rd_order  \\\n",
              "0                      1.333333                      6.866667   \n",
              "1                      2.000000                      5.105263   \n",
              "2                      1.652174                      5.913043   \n",
              "3                      2.666667                      5.407407   \n",
              "4                      2.629630                      5.222222   \n",
              "\n",
              "   tot_defor_2013_lag_3rd_order  tot_defor_2014_lag_3rd_order  \\\n",
              "0                      0.733333                      2.200000   \n",
              "1                      0.526316                      0.947368   \n",
              "2                      4.086957                      4.521739   \n",
              "3                      4.000000                      3.925926   \n",
              "4                      7.592592                      5.370370   \n",
              "\n",
              "   tot_defor_2015_lag_3rd_order  tot_defor_2016_lag_3rd_order  \\\n",
              "0                      4.466667                      9.866667   \n",
              "1                      1.473684                      9.473684   \n",
              "2                      4.956522                      8.695652   \n",
              "3                      3.703704                      5.888889   \n",
              "4                      4.481482                      8.888889   \n",
              "\n",
              "   tot_defor_2017_lag_3rd_order  tot_defor_2018_lag_3rd_order  s  \n",
              "0                      6.600000                      0.800000  1  \n",
              "1                      6.210527                      2.000000  1  \n",
              "2                     11.217392                      5.173913  1  \n",
              "3                     19.629629                      6.518518  1  \n",
              "4                     18.888889                      5.222222  1  \n",
              "\n",
              "[5 rows x 426 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c821f5ff-1399-49f3-9da3-b6daa286847f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>row</th>\n",
              "      <th>col</th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>bean</th>\n",
              "      <th>carrot</th>\n",
              "      <th>cassava</th>\n",
              "      <th>chickpea</th>\n",
              "      <th>citrus</th>\n",
              "      <th>...</th>\n",
              "      <th>tot_defor_2010_lag_3rd_order</th>\n",
              "      <th>tot_defor_2011_lag_3rd_order</th>\n",
              "      <th>tot_defor_2012_lag_3rd_order</th>\n",
              "      <th>tot_defor_2013_lag_3rd_order</th>\n",
              "      <th>tot_defor_2014_lag_3rd_order</th>\n",
              "      <th>tot_defor_2015_lag_3rd_order</th>\n",
              "      <th>tot_defor_2016_lag_3rd_order</th>\n",
              "      <th>tot_defor_2017_lag_3rd_order</th>\n",
              "      <th>tot_defor_2018_lag_3rd_order</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-59.989876</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.800000</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>6.866667</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>4.466667</td>\n",
              "      <td>9.866667</td>\n",
              "      <td>6.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-59.969875</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.052631</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.105263</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.473684</td>\n",
              "      <td>9.473684</td>\n",
              "      <td>6.210527</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-59.949875</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.652174</td>\n",
              "      <td>1.652174</td>\n",
              "      <td>5.913043</td>\n",
              "      <td>4.086957</td>\n",
              "      <td>4.521739</td>\n",
              "      <td>4.956522</td>\n",
              "      <td>8.695652</td>\n",
              "      <td>11.217392</td>\n",
              "      <td>5.173913</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-59.929874</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>200.00000</td>\n",
              "      <td>335.00000</td>\n",
              "      <td>201.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.814815</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>5.407407</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.925926</td>\n",
              "      <td>3.703704</td>\n",
              "      <td>5.888889</td>\n",
              "      <td>19.629629</td>\n",
              "      <td>6.518518</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-59.909874</td>\n",
              "      <td>-10.010125</td>\n",
              "      <td>218.33334</td>\n",
              "      <td>435.83334</td>\n",
              "      <td>216.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>523.5</td>\n",
              "      <td>...</td>\n",
              "      <td>8.296296</td>\n",
              "      <td>2.629630</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>7.592592</td>\n",
              "      <td>5.370370</td>\n",
              "      <td>4.481482</td>\n",
              "      <td>8.888889</td>\n",
              "      <td>18.888889</td>\n",
              "      <td>5.222222</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 426 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c821f5ff-1399-49f3-9da3-b6daa286847f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c821f5ff-1399-49f3-9da3-b6daa286847f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c821f5ff-1399-49f3-9da3-b6daa286847f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Have a look at the data\n",
        "print('Number of rows:', df.shape[0])\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z2ZGZoInoHAV"
      },
      "outputs": [],
      "source": [
        "# Define target (dependent) variable (% forest cover for 2018)\n",
        "strY = 'perc_treecover'\n",
        "\n",
        "\n",
        "# Define a list of features names (explantory variables)\n",
        "lstX = [\n",
        "  'wdpa_2017',\n",
        "  'population_2015',\n",
        "  'chirps_2017',\n",
        "  'maize',\n",
        "  'soy',\n",
        "  'sugarcane',\n",
        "  'perm_water',\n",
        "  'travel_min',\n",
        "  'cropland',\n",
        "  'mean_elev',\n",
        "  'sd_elev',\n",
        "  'near_road',\n",
        " ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRd3KIDXp152"
      },
      "source": [
        "Run OLS on forest cover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bkQjti6z028V"
      },
      "outputs": [],
      "source": [
        "# Get target variable and features\n",
        "Y_all = df[strY]\n",
        "X_all = df.loc[:,lstX]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjHfg6PmxDA",
        "outputId": "efd96642-ed16-4264-f384-66e70df5f965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(normalize=True)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Run OLS using sklearn\n",
        "# We run an regression using sklearn which is one of the most popular \n",
        "# libaries for machine learning \n",
        " \n",
        "# Define model (automatically add a constant and normalize the data) \n",
        "regOls = LinearRegression(fit_intercept=True, normalize=True)\n",
        "# Fit model\n",
        "regOls.fit(X_all, Y_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "toJ0q4YvoEs9",
        "outputId": "f54b994d-89d2-4b1c-96dc-5e1902a26828"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      beta        SE           t   p-value\n",
              "const            -5.831235  1.324112   -4.403882  0.000011\n",
              "wdpa_2017        11.427969  0.156940   72.817276  0.000000\n",
              "population_2015  -0.008522  0.000677  -12.595832  0.000000\n",
              "chirps_2017       0.036649  0.000314  116.867636  0.000000\n",
              "maize             0.019095  0.000159  120.445045  0.000000\n",
              "soy              -0.030865  0.000407  -75.758273  0.000000\n",
              "sugarcane        -0.003199  0.000141  -22.739455  0.000000\n",
              "perm_water      -21.908606  1.184469  -18.496556  0.000000\n",
              "travel_min        0.019945  0.000139  143.456389  0.000000\n",
              "cropland         -1.174515  0.540421   -2.173335  0.029755\n",
              "mean_elev        -0.023366  0.000449  -51.999590  0.000000\n",
              "sd_elev           0.417715  0.004348   96.080777  0.000000\n",
              "near_road         0.069442  0.001149   60.442058  0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c92013c5-a1ff-4dca-a975-cb22fb6014af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beta</th>\n",
              "      <th>SE</th>\n",
              "      <th>t</th>\n",
              "      <th>p-value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>const</th>\n",
              "      <td>-5.831235</td>\n",
              "      <td>1.324112</td>\n",
              "      <td>-4.403882</td>\n",
              "      <td>0.000011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wdpa_2017</th>\n",
              "      <td>11.427969</td>\n",
              "      <td>0.156940</td>\n",
              "      <td>72.817276</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>population_2015</th>\n",
              "      <td>-0.008522</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>-12.595832</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chirps_2017</th>\n",
              "      <td>0.036649</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>116.867636</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maize</th>\n",
              "      <td>0.019095</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>120.445045</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soy</th>\n",
              "      <td>-0.030865</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>-75.758273</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sugarcane</th>\n",
              "      <td>-0.003199</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>-22.739455</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perm_water</th>\n",
              "      <td>-21.908606</td>\n",
              "      <td>1.184469</td>\n",
              "      <td>-18.496556</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>travel_min</th>\n",
              "      <td>0.019945</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>143.456389</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cropland</th>\n",
              "      <td>-1.174515</td>\n",
              "      <td>0.540421</td>\n",
              "      <td>-2.173335</td>\n",
              "      <td>0.029755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean_elev</th>\n",
              "      <td>-0.023366</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>-51.999590</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sd_elev</th>\n",
              "      <td>0.417715</td>\n",
              "      <td>0.004348</td>\n",
              "      <td>96.080777</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>near_road</th>\n",
              "      <td>0.069442</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>60.442058</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c92013c5-a1ff-4dca-a975-cb22fb6014af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c92013c5-a1ff-4dca-a975-cb22fb6014af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c92013c5-a1ff-4dca-a975-cb22fb6014af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from scipy.stats import norm\n",
        "# View regression results\n",
        "# Note: Sklearn is not econometric package and does not provide our classical \n",
        "# regression table output. This is not what the machine learning community\n",
        "# looks at. However, we can calculate these things manually.\n",
        "\n",
        "# Get coef\n",
        "coefs = np.hstack((regOls.intercept_,regOls.coef_))\n",
        "\n",
        "N = Y_all.shape[0]\n",
        "K = coefs.shape[0]\n",
        "\n",
        "# Get predicted values y hat\n",
        "Y_hat = regOls.predict(X_all)\n",
        "# Get errors\n",
        "err = Y_all-Y_hat\n",
        "# Get standard error of regression\n",
        "sig2 = (err.transpose() @ err) / (N-K)\n",
        "sig2\n",
        "\n",
        "# Add constant to X_all (sklearn did this automatically)\n",
        "Xc_all = np.insert(np.array(X_all), 0, 1, axis = 1)\n",
        "\n",
        "# Covariance matrix for coef\n",
        "VarBeta = sig2 * np.linalg.inv(Xc_all.T @ Xc_all)\n",
        "# Standard error of coef\n",
        "se = np.sqrt(np.diag(VarBeta))\n",
        "# t-values\n",
        "t =  coefs/se\n",
        "# p-values  \n",
        "p = (1 - norm.cdf(abs(t))) * 2\n",
        "# Prepare df as output\n",
        "resOls = pd.DataFrame(coefs,index=['const']+lstX,columns=['beta'])\n",
        "resOls['SE'] = se\n",
        "resOls['t'] = t\n",
        "resOls['p-value'] = p\n",
        "resOls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "MUQqGRpAYG1p",
        "outputId": "369eabd8-81ca-42f2-c600-462de2ee7dbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:         perc_treecover   R-squared:                       0.448\n",
              "Model:                            OLS   Adj. R-squared:                  0.448\n",
              "Method:                 Least Squares   F-statistic:                 1.688e+04\n",
              "Date:                Mon, 19 Sep 2022   Prob (F-statistic):               0.00\n",
              "Time:                        12:40:52   Log-Likelihood:            -1.1677e+06\n",
              "No. Observations:              249940   AIC:                         2.335e+06\n",
              "Df Residuals:                  249927   BIC:                         2.336e+06\n",
              "Df Model:                          12                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "===================================================================================\n",
              "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
              "-----------------------------------------------------------------------------------\n",
              "const              -5.8312      1.324     -4.404      0.000      -8.426      -3.236\n",
              "wdpa_2017          11.4280      0.157     72.817      0.000      11.120      11.736\n",
              "population_2015    -0.0085      0.001    -12.596      0.000      -0.010      -0.007\n",
              "chirps_2017         0.0366      0.000    116.868      0.000       0.036       0.037\n",
              "maize               0.0191      0.000    120.445      0.000       0.019       0.019\n",
              "soy                -0.0309      0.000    -75.758      0.000      -0.032      -0.030\n",
              "sugarcane          -0.0032      0.000    -22.739      0.000      -0.003      -0.003\n",
              "perm_water        -21.9086      1.184    -18.497      0.000     -24.230     -19.587\n",
              "travel_min          0.0199      0.000    143.456      0.000       0.020       0.020\n",
              "cropland           -1.1745      0.540     -2.173      0.030      -2.234      -0.115\n",
              "mean_elev          -0.0234      0.000    -52.000      0.000      -0.024      -0.022\n",
              "sd_elev             0.4177      0.004     96.081      0.000       0.409       0.426\n",
              "near_road           0.0694      0.001     60.442      0.000       0.067       0.072\n",
              "==============================================================================\n",
              "Omnibus:                     2376.776   Durbin-Watson:                   0.468\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1780.521\n",
              "Skew:                           0.113   Prob(JB):                         0.00\n",
              "Kurtosis:                       2.654   Cond. No.                     1.27e+05\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The condition number is large, 1.27e+05. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>     <td>perc_treecover</td>  <th>  R-squared:         </th>  <td>   0.448</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.448</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.688e+04</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Mon, 19 Sep 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>12:40:52</td>     <th>  Log-Likelihood:    </th> <td>-1.1677e+06</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>249940</td>      <th>  AIC:               </th>  <td>2.335e+06</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>249927</td>      <th>  BIC:               </th>  <td>2.336e+06</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>    12</td>      <th>                     </th>      <td> </td>     \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th>           <td>   -5.8312</td> <td>    1.324</td> <td>   -4.404</td> <td> 0.000</td> <td>   -8.426</td> <td>   -3.236</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>wdpa_2017</th>       <td>   11.4280</td> <td>    0.157</td> <td>   72.817</td> <td> 0.000</td> <td>   11.120</td> <td>   11.736</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>population_2015</th> <td>   -0.0085</td> <td>    0.001</td> <td>  -12.596</td> <td> 0.000</td> <td>   -0.010</td> <td>   -0.007</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>chirps_2017</th>     <td>    0.0366</td> <td>    0.000</td> <td>  116.868</td> <td> 0.000</td> <td>    0.036</td> <td>    0.037</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>maize</th>           <td>    0.0191</td> <td>    0.000</td> <td>  120.445</td> <td> 0.000</td> <td>    0.019</td> <td>    0.019</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>soy</th>             <td>   -0.0309</td> <td>    0.000</td> <td>  -75.758</td> <td> 0.000</td> <td>   -0.032</td> <td>   -0.030</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>sugarcane</th>       <td>   -0.0032</td> <td>    0.000</td> <td>  -22.739</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.003</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>perm_water</th>      <td>  -21.9086</td> <td>    1.184</td> <td>  -18.497</td> <td> 0.000</td> <td>  -24.230</td> <td>  -19.587</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>travel_min</th>      <td>    0.0199</td> <td>    0.000</td> <td>  143.456</td> <td> 0.000</td> <td>    0.020</td> <td>    0.020</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>cropland</th>        <td>   -1.1745</td> <td>    0.540</td> <td>   -2.173</td> <td> 0.030</td> <td>   -2.234</td> <td>   -0.115</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>mean_elev</th>       <td>   -0.0234</td> <td>    0.000</td> <td>  -52.000</td> <td> 0.000</td> <td>   -0.024</td> <td>   -0.022</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>sd_elev</th>         <td>    0.4177</td> <td>    0.004</td> <td>   96.081</td> <td> 0.000</td> <td>    0.409</td> <td>    0.426</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>near_road</th>       <td>    0.0694</td> <td>    0.001</td> <td>   60.442</td> <td> 0.000</td> <td>    0.067</td> <td>    0.072</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>2376.776</td> <th>  Durbin-Watson:     </th> <td>   0.468</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1780.521</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td> 0.113</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td> 2.654</td>  <th>  Cond. No.          </th> <td>1.27e+05</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.27e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# To confirm the results we can use the OLS function in the statsmodel libary.\n",
        "# This is more a statistical libary, not typically used for machine learning, \n",
        "# but providing our typicall regression output\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "olsStats = sm.OLS(Y_all, np.insert(np.array(X_all), 0, 1, axis = 1))\n",
        "# Set the names of the explanatory variables\n",
        "olsStats.data.xnames = ['const']+lstX\n",
        "olsStats_result = olsStats.fit()\n",
        "olsStats_result.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "sQjoOnOt3_7p",
        "outputId": "dbb1407e-b3c8-4281-ccb0-0752646e57de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean squared error:  669.092064194029\n",
            "Coefficient of determination:  0.44772019128724716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Now we want to see how good our OLS regression performs in sample\n",
        "# i.e. on the training data itself\n",
        "\n",
        "# Make predictions for the same dataset X_all\n",
        "Y_hat = regOls.predict(X_all)\n",
        "\n",
        "# The mean squared error\n",
        "mse_ols_sklearn  = mean_squared_error(Y_all,Y_hat)\n",
        "print('\\nMean squared error: ',mse_ols_sklearn)\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "R2_ols_sklearn = r2_score(Y_all,Y_hat)\n",
        "print('Coefficient of determination: ',R2_ols_sklearn)\n",
        "\n",
        "# plot Y vs Y-hat\n",
        "h = sns.jointplot(Y_hat, Y_all, kind=\"hex\")\n",
        "h.set_axis_labels('Y predicted', 'Y true');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh5EZajJsA8N"
      },
      "source": [
        "Now we explore what consequences it has if a model overfits. For this we generate interaction and squared terms and compare model performance in a training and test set approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9MgnKOu0M5xH"
      },
      "outputs": [],
      "source": [
        "# Set a random number seed such that everybody has the same \"random\" split \n",
        "# of the data\n",
        "np.random.seed(111)\n",
        "# Split the data into train and test data using sklearn train_test_split object\n",
        "#   (see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n",
        "#   Note: This randomly split the data in 80% train and 20% test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oZ3is0GFj8k8"
      },
      "outputs": [],
      "source": [
        "# In order to better illustrate the overfitting effects we \n",
        "# artificially reduce the training data size.  Why do we do this? In general \n",
        "# a larger sample size helps to reduce overfitting problems. With a smaller \n",
        "# sample size polynomials of order 2 of our variables are sufficient to show \n",
        "# the effects. With the full sample size we would need to consider much higher \n",
        "# polynomials to show the same effects. \n",
        "N = 2000\n",
        "X_train = X_train.iloc[:N,:]\n",
        "Y_train = Y_train.iloc[:N]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZpKA77RMr2u",
        "outputId": "67ee9634-e249-4954-ba47-5ce1ac08e28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of avaliable features 91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Use an sklearn function to generate polynomials of order 2 \n",
        "# (square terms and interaction terms)\n",
        "# (see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures.get_feature_names)\n",
        "poly = PolynomialFeatures(2)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "print('Total number of avaliable features',X_train_poly.shape[1])\n",
        "lstFeatures = poly.get_feature_names()\n",
        "# Show feature names\n",
        "# list(lstFeatures)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3dKToEAbrNXk",
        "outputId": "2b89c81d-1b62-424f-9959-26683f4a7d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Number of X Variables New Feature    MSE Train      MSE Test  \\\n",
              "numVar_0                     1.0           1  1216.525636   1213.005815   \n",
              "numVar_1                     2.0          x0  1092.857849   1102.641178   \n",
              "numVar_2                     3.0          x1  1091.452623   1101.598429   \n",
              "numVar_3                     4.0          x2   956.060772    981.976805   \n",
              "numVar_4                     5.0          x3   913.715950    929.376214   \n",
              "...                          ...         ...          ...           ...   \n",
              "numVar_86                   87.0      x9 x10   530.718203  42927.052274   \n",
              "numVar_87                   88.0      x9 x11   523.025791  43000.110736   \n",
              "numVar_88                   89.0       x10^2   518.977085  37915.027224   \n",
              "numVar_89                   90.0     x10 x11   517.769904  37210.556117   \n",
              "numVar_90                   91.0       x11^2   515.200839  40050.104395   \n",
              "\n",
              "           R2 train    R2 test  \n",
              "numVar_0   0.000000  -0.000181  \n",
              "numVar_1   0.101657   0.090820  \n",
              "numVar_2   0.102812   0.091680  \n",
              "numVar_3   0.214106   0.190314  \n",
              "numVar_4   0.248914   0.233685  \n",
              "...             ...        ...  \n",
              "numVar_86  0.563743 -34.395385  \n",
              "numVar_87  0.570066 -34.455625  \n",
              "numVar_88  0.573394 -30.262733  \n",
              "numVar_89  0.574386 -29.681864  \n",
              "numVar_90  0.576498 -32.023205  \n",
              "\n",
              "[91 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-793ae1c1-0bb3-4393-8ddf-9f5a68811fbf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number of X Variables</th>\n",
              "      <th>New Feature</th>\n",
              "      <th>MSE Train</th>\n",
              "      <th>MSE Test</th>\n",
              "      <th>R2 train</th>\n",
              "      <th>R2 test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>numVar_0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1216.525636</td>\n",
              "      <td>1213.005815</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.000181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>x0</td>\n",
              "      <td>1092.857849</td>\n",
              "      <td>1102.641178</td>\n",
              "      <td>0.101657</td>\n",
              "      <td>0.090820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>x1</td>\n",
              "      <td>1091.452623</td>\n",
              "      <td>1101.598429</td>\n",
              "      <td>0.102812</td>\n",
              "      <td>0.091680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>x2</td>\n",
              "      <td>956.060772</td>\n",
              "      <td>981.976805</td>\n",
              "      <td>0.214106</td>\n",
              "      <td>0.190314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>x3</td>\n",
              "      <td>913.715950</td>\n",
              "      <td>929.376214</td>\n",
              "      <td>0.248914</td>\n",
              "      <td>0.233685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_86</th>\n",
              "      <td>87.0</td>\n",
              "      <td>x9 x10</td>\n",
              "      <td>530.718203</td>\n",
              "      <td>42927.052274</td>\n",
              "      <td>0.563743</td>\n",
              "      <td>-34.395385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_87</th>\n",
              "      <td>88.0</td>\n",
              "      <td>x9 x11</td>\n",
              "      <td>523.025791</td>\n",
              "      <td>43000.110736</td>\n",
              "      <td>0.570066</td>\n",
              "      <td>-34.455625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_88</th>\n",
              "      <td>89.0</td>\n",
              "      <td>x10^2</td>\n",
              "      <td>518.977085</td>\n",
              "      <td>37915.027224</td>\n",
              "      <td>0.573394</td>\n",
              "      <td>-30.262733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_89</th>\n",
              "      <td>90.0</td>\n",
              "      <td>x10 x11</td>\n",
              "      <td>517.769904</td>\n",
              "      <td>37210.556117</td>\n",
              "      <td>0.574386</td>\n",
              "      <td>-29.681864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>numVar_90</th>\n",
              "      <td>91.0</td>\n",
              "      <td>x11^2</td>\n",
              "      <td>515.200839</td>\n",
              "      <td>40050.104395</td>\n",
              "      <td>0.576498</td>\n",
              "      <td>-32.023205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>91 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-793ae1c1-0bb3-4393-8ddf-9f5a68811fbf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-793ae1c1-0bb3-4393-8ddf-9f5a68811fbf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-793ae1c1-0bb3-4393-8ddf-9f5a68811fbf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# To explore overfitting we now run an small simulation...\n",
        "# we squentially increase our model complexity by adding on additional features. \n",
        "# In each case we estimate the model and obtain models stats\n",
        "# for the train and test set.\n",
        "\n",
        "# Prepare a dataframe to hold the results\n",
        "res = pd.DataFrame()\n",
        "\n",
        "# Each iteration add in one  additional variables\n",
        "for numVar in range(0,X_train_poly.shape[1]):\n",
        "\n",
        "  # Define the feature set for the iteration \n",
        "  X_train_subSet = X_train_poly[:,:numVar+1]\n",
        "  X_test_subSet = X_test_poly[:,:numVar+1]\n",
        "  \n",
        "  # Create linear regression object\n",
        "  regOls = LinearRegression(normalize=True)\n",
        "\n",
        "  # Train the model using the training sets\n",
        "  regOls.fit(X_train_subSet, Y_train)\n",
        "\n",
        "  # Get predicted values\n",
        "  Y_hat_train = regOls.predict(X_train_subSet)\n",
        "  Y_hat_test = regOls.predict(X_test_subSet)\n",
        "\n",
        "  # Store model stats \n",
        "  res.loc[f\"numVar_{numVar}\",'Number of X Variables'] = X_train_subSet.shape[1]\n",
        "  res.loc[f\"numVar_{numVar}\",'New Feature'] = lstFeatures[numVar]\n",
        "\n",
        "  # The mean squared error\n",
        "  res.loc[f\"numVar_{numVar}\",'MSE Train'] = mean_squared_error(Y_train,Y_hat_train)\n",
        "  res.loc[f\"numVar_{numVar}\",'MSE Test'] = mean_squared_error(Y_test,Y_hat_test)\n",
        "  # The coefficient of determination: 1 is perfect prediction\n",
        "  res.loc[f\"numVar_{numVar}\",'R2 train'] = r2_score(Y_train,Y_hat_train)\n",
        "  res.loc[f\"numVar_{numVar}\",'R2 test'] = r2_score(Y_test,Y_hat_test)\n",
        "\n",
        "\n",
        "# => The column \"New Feature\" says which feature was added in this iteration \n",
        "#    on top of all the other added before, starting with only a constant\n",
        "# => Have a look how R2 and MSE develops in the train and test set when \n",
        "#    increasing model complexity.\n",
        "res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J13_mhypxJ25",
        "outputId": "0de7a1ad-f38e-4715-fded-0a48ef1dc386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results from statsmodel:\n",
            "R2 0.44772019128724727\n",
            "MSE 669.1268671438283\n",
            "\n",
            "Results from sklearn:\n",
            "Mean squared error:  669.092064194029\n",
            "Coefficient of determination:  0.44772019128724716\n",
            "\n",
            "Results from row \"numVar_12\" from the \"res\" dataframe\n",
            "Number of X Variables          13.0\n",
            "New Feature                     x11\n",
            "MSE Train                655.299137\n",
            "MSE Test                 672.795379\n",
            "R2 train                   0.461336\n",
            "R2 test                    0.445248\n",
            "Name: numVar_12, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Before we continue lets check if our simulation is correct, by comparing\n",
        "# row \"numVar_12\" from \"res\" with our statsmodel and sklearn that we \n",
        "# have use above\n",
        "print('Results from statsmodel:')\n",
        "print('R2',olsStats_result.rsquared)\n",
        "print('MSE',olsStats_result.mse_resid)\n",
        "\n",
        "print('\\nResults from sklearn:')\n",
        "print('Mean squared error: ',mse_ols_sklearn)\n",
        "print('Coefficient of determination: ',R2_ols_sklearn)\n",
        "\n",
        "# Check that the results match our \"res\" data frame in row \"numVar_12\", which is\n",
        "# a linear model with all explanatory variables\n",
        "print('\\nResults from row \"numVar_12\" from the \"res\" dataframe')\n",
        "print(res.loc['numVar_12',:])\n",
        "\n",
        "# => Note that in the simulation our X_train is a much smaller subsample \n",
        "#   of the full data set. Hence, we have some small variation but overall \n",
        "#   the result is very comparable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "fI2i8omgj82L",
        "outputId": "f567f54a-0fa4-46ab-f84a-d00864929325"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Now back to our simulation results. To inspect the overfitting of the model\n",
        "# lets plot R2 and MSE against the number of variables in the model.\n",
        "\n",
        "# Start with R2\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(res['Number of X Variables'],res['R2 train'],label='train')\n",
        "ax.plot(res['Number of X Variables'],res['R2 test'],label='test')\n",
        "ax.set_ylabel('R2')\n",
        "ax.set_xlabel('number of variables')\n",
        "ax.set_ylim(0,1)\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "ax.legend();\n",
        "\n",
        "# => Note that in same cases our R2 in the test set is negative, which means\n",
        "#    that our model is completely off..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "zpJGleUmtu-U",
        "outputId": "54800cdb-2db0-4a7a-c72d-d4ed1c3b48f3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# ... and MSE\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(res['Number of X Variables'],res['MSE Train'],label='train')\n",
        "ax.plot(res['Number of X Variables'],res['MSE Test'],label='test')\n",
        "ax.set_ylabel('MSE')\n",
        "ax.set_xlabel('number of variables')\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8Z-gJtvIejP8"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Question to discuss in the group:\n",
        "# =====================================\n",
        "\n",
        "# What can you conclude from the plots?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX9o1ITDyx_s",
        "outputId": "71ac2dbc-6f0e-42f1-fc82-1d8ce6bfc019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1',\n",
              " 'x0',\n",
              " 'x1',\n",
              " 'x2',\n",
              " 'x3',\n",
              " 'x4',\n",
              " 'x5',\n",
              " 'x6',\n",
              " 'x7',\n",
              " 'x8',\n",
              " 'x9',\n",
              " 'x10',\n",
              " 'x11',\n",
              " 'x0^2',\n",
              " 'x0 x1',\n",
              " 'x0 x2',\n",
              " 'x0 x3',\n",
              " 'x0 x4',\n",
              " 'x0 x5',\n",
              " 'x0 x6',\n",
              " 'x0 x7',\n",
              " 'x0 x8',\n",
              " 'x0 x9']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Now, we can find the \"best\" specification from our simulation \n",
        "# by checking where R2 is highest in test set\n",
        "iRes = res.loc[res['R2 test']==res['R2 test'].max(),:]\n",
        "lstFeatures[:int(iRes['Number of X Variables'])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4cAlNildepgY"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Question to discuss in the group:\n",
        "# =====================================\n",
        "\n",
        "# Why might this model specification strategy not be ideal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy4hWsgdQ6-f"
      },
      "source": [
        "### Let's see how Lasso could be used here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qoVC3W4BRq-g"
      },
      "outputs": [],
      "source": [
        "# The idea is that instead of adding in one variable at a time, we use all\n",
        "# and let Lasso decide which ones to use. \n",
        "\n",
        "# It turns out that in order to get better results we need to make \n",
        "# the problem one step simpler for Lasso. If we use all polynomial terms \n",
        "# Lasso has convergence issues, at least with the restricted sample size \n",
        "# that we used above. Apparently the problem is that our variables are too \n",
        "# correlated. However, we can also illustrate the apporoach by using only linear \n",
        "# and square terms (excluding the interaction terms).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GTWCqeowEQ36"
      },
      "outputs": [],
      "source": [
        "# Get only the linear and square terms from the polynomials\n",
        "idxSq = list(range(0,13))+[lstFeatures.index(c) for c in lstFeatures if '^2' in c]\n",
        "lstColNames = [lstFeatures[i] for i in idxSq]\n",
        "X_trainSq = X_train_poly[:,idxSq]\n",
        "X_testSq = X_test_poly[:,idxSq]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dgptOE8SvOs4"
      },
      "outputs": [],
      "source": [
        "# Get actual names for features instead of x1,...,x11\n",
        "# this is required for a plot further down...\n",
        "sCols = pd.Series(lstColNames)\n",
        "sCols = sCols.replace({'\\^2':'_sq'},regex=True)\n",
        "sCols = sCols.replace({'1':'const'},regex=False)\n",
        "dctReplace = {f\"x{i}\":lstX[i] for i in range(10,len(lstX))}\n",
        "sCols = sCols.replace(dctReplace,regex=True)\n",
        "dctReplace = {f\"x{i}\":lstX[i] for i in range(0,10)}\n",
        "sCols = sCols.replace(dctReplace,regex=True)\n",
        "lstColNames = sCols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMT0EcKm-49Y",
        "outputId": "8501e55e-dd25-4439-8b0b-d3bd13fb396c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.52587891e-05, 2.61610102e-05, 4.48527372e-05, 7.68994781e-05,\n",
              "       1.31843230e-04, 2.26043631e-04, 3.87549085e-04, 6.64448242e-04,\n",
              "       1.13918852e-03, 1.95312500e-03])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Create a list with alpha (= lambda in the lecture) values that we want to test for our Lasso estimation\n",
        "lstAlpha = np.logspace(-16, -9, num = 10, base = 2)\n",
        "lstAlpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pqPfDYY-GtEi",
        "outputId": "894c69d2-0b0e-49b3-9308-b4e74add86b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.52587890625e-05\n",
            "2.6161010218904826e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.048e+05, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.215e+05, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.485273719102779e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.207e+04, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.689947814299766e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+04, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.102e+04, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00013184322984525218\n",
            "0.00022604363092951433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.064e+03, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.452e+03, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.000387549084953174\n",
            "0.0006644482422726451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.843e+02, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0011391885152110507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.646e+02, tolerance: 2.433e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001953125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               MSE Train    MSE Test  R2 train   R2 test  \\\n",
              "lasso_1.52587890625e-05       593.657261  627.688094  0.512006  0.482441   \n",
              "lasso_2.6161010218904826e-05  593.660229  627.619396  0.512004  0.482498   \n",
              "lasso_4.485273719102779e-05   593.668860  627.505851  0.511996  0.482592   \n",
              "lasso_7.689947814299766e-05   593.694066  627.323643  0.511976  0.482742   \n",
              "lasso_0.00013184322984525218  593.767876  627.047868  0.511915  0.482969   \n",
              "lasso_0.00022604363092951433  593.984358  626.682714  0.511737  0.483270   \n",
              "lasso_0.000387549084953174    594.619874  626.373121  0.511215  0.483526   \n",
              "lasso_0.0006644482422726451   595.720439  625.881953  0.510310  0.483931   \n",
              "lasso_0.0011391885152110507   598.108087  625.772255  0.508347  0.484021   \n",
              "lasso_0.001953125             604.935875  629.384882  0.502735  0.481042   \n",
              "\n",
              "                                 alpha  \n",
              "lasso_1.52587890625e-05       0.000015  \n",
              "lasso_2.6161010218904826e-05  0.000026  \n",
              "lasso_4.485273719102779e-05   0.000045  \n",
              "lasso_7.689947814299766e-05   0.000077  \n",
              "lasso_0.00013184322984525218  0.000132  \n",
              "lasso_0.00022604363092951433  0.000226  \n",
              "lasso_0.000387549084953174    0.000388  \n",
              "lasso_0.0006644482422726451   0.000664  \n",
              "lasso_0.0011391885152110507   0.001139  \n",
              "lasso_0.001953125             0.001953  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d201acc2-aa9a-4cca-b93b-a12e62f11371\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSE Train</th>\n",
              "      <th>MSE Test</th>\n",
              "      <th>R2 train</th>\n",
              "      <th>R2 test</th>\n",
              "      <th>alpha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lasso_1.52587890625e-05</th>\n",
              "      <td>593.657261</td>\n",
              "      <td>627.688094</td>\n",
              "      <td>0.512006</td>\n",
              "      <td>0.482441</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_2.6161010218904826e-05</th>\n",
              "      <td>593.660229</td>\n",
              "      <td>627.619396</td>\n",
              "      <td>0.512004</td>\n",
              "      <td>0.482498</td>\n",
              "      <td>0.000026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_4.485273719102779e-05</th>\n",
              "      <td>593.668860</td>\n",
              "      <td>627.505851</td>\n",
              "      <td>0.511996</td>\n",
              "      <td>0.482592</td>\n",
              "      <td>0.000045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_7.689947814299766e-05</th>\n",
              "      <td>593.694066</td>\n",
              "      <td>627.323643</td>\n",
              "      <td>0.511976</td>\n",
              "      <td>0.482742</td>\n",
              "      <td>0.000077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_0.00013184322984525218</th>\n",
              "      <td>593.767876</td>\n",
              "      <td>627.047868</td>\n",
              "      <td>0.511915</td>\n",
              "      <td>0.482969</td>\n",
              "      <td>0.000132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_0.00022604363092951433</th>\n",
              "      <td>593.984358</td>\n",
              "      <td>626.682714</td>\n",
              "      <td>0.511737</td>\n",
              "      <td>0.483270</td>\n",
              "      <td>0.000226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_0.000387549084953174</th>\n",
              "      <td>594.619874</td>\n",
              "      <td>626.373121</td>\n",
              "      <td>0.511215</td>\n",
              "      <td>0.483526</td>\n",
              "      <td>0.000388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_0.0006644482422726451</th>\n",
              "      <td>595.720439</td>\n",
              "      <td>625.881953</td>\n",
              "      <td>0.510310</td>\n",
              "      <td>0.483931</td>\n",
              "      <td>0.000664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_0.0011391885152110507</th>\n",
              "      <td>598.108087</td>\n",
              "      <td>625.772255</td>\n",
              "      <td>0.508347</td>\n",
              "      <td>0.484021</td>\n",
              "      <td>0.001139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lasso_0.001953125</th>\n",
              "      <td>604.935875</td>\n",
              "      <td>629.384882</td>\n",
              "      <td>0.502735</td>\n",
              "      <td>0.481042</td>\n",
              "      <td>0.001953</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d201acc2-aa9a-4cca-b93b-a12e62f11371')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d201acc2-aa9a-4cca-b93b-a12e62f11371 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d201acc2-aa9a-4cca-b93b-a12e62f11371');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Now loop over the alphas, run a Lasso estimation and get the model stats \n",
        "# Note the alphas here are just the same as the penalty (lambda)\n",
        "\n",
        "# Create a dataframe to hold model stats\n",
        "resLasso = pd.DataFrame()\n",
        "# Create a dataframe to hold coef\n",
        "resCoef = pd.DataFrame(columns=lstColNames)\n",
        "\n",
        "# Loop over a range of alpha value\n",
        "for alpha in lstAlpha:\n",
        "  print(alpha)\n",
        "  # Estimate Lasso\n",
        "  if alpha >0:\n",
        "    modLasso = Lasso(normalize=True, fit_intercept=True, alpha=alpha)\n",
        "  else:  \n",
        "    # in case of alpha = 0 use LinearRegression as recommended by sklearn\n",
        "    modLasso = LinearRegression(normalize=True, fit_intercept=True)\n",
        "  modLasso.fit(X_trainSq, Y_train)\n",
        "\n",
        "  # Get predicted values\n",
        "  Y_hat_train = modLasso.predict(X_trainSq)\n",
        "  Y_hat_test = modLasso.predict(X_testSq)\n",
        "\n",
        "  # Get model stats\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'MSE Train'] = mean_squared_error(Y_train,Y_hat_train)\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'MSE Test'] = mean_squared_error(Y_test,Y_hat_test)\n",
        "  # The coefficient of determination: 1 is perfect prediction\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'R2 train'] = r2_score(Y_train,Y_hat_train)\n",
        "  resLasso.loc[f\"lasso_{alpha}\",'R2 test'] = r2_score(Y_test,Y_hat_test)\n",
        "  \n",
        "  resLasso.loc[f\"lasso_{alpha}\",'alpha'] = alpha\n",
        "\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}','alpha',] = alpha\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}',lstColNames] = modLasso.coef_.transpose()\n",
        "\n",
        "\n",
        "# => As you will see in the output, there are some values of alpha for which \n",
        "#   Lasso does not converge. This is not ideal but for our results we can \n",
        "#   ignore this for now.\n",
        "# => Again check how MSE and R2 developed for varying values of alpha\n",
        "\n",
        "resLasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1XJUewDK9Z2",
        "outputId": "9e9dc243-f203-43ee-b69a-6c610320c7d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \"best\" (from those we tried in the simulations) is:  0.0011391885152110507\n"
          ]
        }
      ],
      "source": [
        "# Get the \"best\" alpha by checking where MSE is lowest in test set \n",
        "# (or R2 is highest)\n",
        "iRes = resLasso.loc[resLasso['R2 test']==resLasso['R2 test'].max(),:]\n",
        "alphaBest = resLasso.loc[resLasso['R2 test']==resLasso['R2 test'].max(),'alpha'][0]\n",
        "print('The \"best\" (from those we tried in the simulations) is: ', alphaBest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "kEqYSwBvIDqD",
        "outputId": "7d6327b4-6888-4628-9126-0d225c183765"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot result\n",
        "\n",
        "# Define range of Y that should be shown in plot\n",
        "rangeYLow = resLasso['MSE Train'].min()*0.99\n",
        "rangeYHigh = resLasso['MSE Test'].max()*1.01\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(resLasso['alpha'],resLasso['MSE Train'],label='train')\n",
        "ax.plot(resLasso['alpha'],resLasso['MSE Test'],label='test')\n",
        "ax.plot([alphaBest,alphaBest],[rangeYLow,rangeYHigh],label='\"best\" alpha',linestyle=':',color='black')\n",
        "ax.set_ylabel('MSE')\n",
        "ax.set_xlabel('alpha')\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "fig.autofmt_xdate(rotation=45)\n",
        "ax.set_ylim(rangeYLow,rangeYHigh)\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat the simulation with a slightly larger alpha range\n",
        "# in order to prepare a plot with the estimated coefficients against alpha \n",
        "# Df to hold coef\n",
        "resCoef = pd.DataFrame(columns=lstColNames)\n",
        "lstAlphaNew = np.logspace(-16, 0, num = 10, base = 2)\n",
        "\n",
        "# Loop over a range of alpha value\n",
        "for alpha in lstAlphaNew:\n",
        "  # Estimate Lasso\n",
        "  if alpha >0:\n",
        "    modLasso = Lasso(normalize=True, fit_intercept=True, alpha=alpha)\n",
        "  else:  \n",
        "    # in case of alpha = 0 use LinearRegression as recommended by sklearn\n",
        "    modLasso = LinearRegression(normalize=True, fit_intercept=True)\n",
        "  modLasso.fit(X_trainSq, Y_train)\n",
        "\n",
        "  # Get predicted values\n",
        "  Y_hat_train = modLasso.predict(X_trainSq)\n",
        "  Y_hat_test = modLasso.predict(X_testSq)\n",
        "\n",
        "  # Get model stats\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}','MSE Train'] = mean_squared_error(Y_train,Y_hat_train)\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}','MSE Test'] = mean_squared_error(Y_test,Y_hat_test)\n",
        "\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}','alpha',] = alpha\n",
        "  resCoef.loc[f'beta_hat_alpha{alpha}',lstColNames] = modLasso.coef_.transpose()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZn_Q2zjx7Bo",
        "outputId": "5bd0b32c-747e-4580-ef7a-d840e29d98a3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:\n",
            "\n",
            "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.048e+05, tolerance: 2.433e+02\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:\n",
            "\n",
            "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.991e+04, tolerance: 2.433e+02\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:\n",
            "\n",
            "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.276e+03, tolerance: 2.433e+02\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning:\n",
            "\n",
            "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.760e+02, tolerance: 2.433e+02\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning:\n",
            "\n",
            "'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resCoef.loc[:,['MSE Train', 'MSE Test', 'alpha']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "jNX2BrmTx8lP",
        "outputId": "b7fc0f2c-5516-4367-a49d-5251370bdbf6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        MSE Train     MSE Test     alpha\n",
              "beta_hat_alpha1.52587890625e-05        593.657261   627.688094  0.000015\n",
              "beta_hat_alpha5.232202043780965e-05    593.673560   627.461976  0.000052\n",
              "beta_hat_alpha0.00017941094876411116   593.862992   626.846480  0.000179\n",
              "beta_hat_alpha0.0006151958251439805    595.539251   625.984011  0.000615\n",
              "beta_hat_alpha0.002109491677524035     606.445968   630.443223  0.002109\n",
              "beta_hat_alpha0.0072333961897444584    623.368980   642.720501  0.007233\n",
              "beta_hat_alpha0.024803141437003108     671.346003   687.582713  0.024803\n",
              "beta_hat_alpha0.08504937501089847      761.687398   777.474703  0.085049\n",
              "beta_hat_alpha0.29163225989402897     1033.612108  1037.566788  0.291632\n",
              "beta_hat_alpha1.0                     1216.525636  1213.005815  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1704dd67-a27d-48d6-a4ec-e86661551725\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSE Train</th>\n",
              "      <th>MSE Test</th>\n",
              "      <th>alpha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha1.52587890625e-05</th>\n",
              "      <td>593.657261</td>\n",
              "      <td>627.688094</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha5.232202043780965e-05</th>\n",
              "      <td>593.673560</td>\n",
              "      <td>627.461976</td>\n",
              "      <td>0.000052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha0.00017941094876411116</th>\n",
              "      <td>593.862992</td>\n",
              "      <td>626.846480</td>\n",
              "      <td>0.000179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha0.0006151958251439805</th>\n",
              "      <td>595.539251</td>\n",
              "      <td>625.984011</td>\n",
              "      <td>0.000615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha0.002109491677524035</th>\n",
              "      <td>606.445968</td>\n",
              "      <td>630.443223</td>\n",
              "      <td>0.002109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha0.0072333961897444584</th>\n",
              "      <td>623.368980</td>\n",
              "      <td>642.720501</td>\n",
              "      <td>0.007233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha0.024803141437003108</th>\n",
              "      <td>671.346003</td>\n",
              "      <td>687.582713</td>\n",
              "      <td>0.024803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha0.08504937501089847</th>\n",
              "      <td>761.687398</td>\n",
              "      <td>777.474703</td>\n",
              "      <td>0.085049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha0.29163225989402897</th>\n",
              "      <td>1033.612108</td>\n",
              "      <td>1037.566788</td>\n",
              "      <td>0.291632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>beta_hat_alpha1.0</th>\n",
              "      <td>1216.525636</td>\n",
              "      <td>1213.005815</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1704dd67-a27d-48d6-a4ec-e86661551725')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1704dd67-a27d-48d6-a4ec-e86661551725 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1704dd67-a27d-48d6-a4ec-e86661551725');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "wVGZFxZfpG6x",
        "outputId": "96e706d7-0cb8-4d76-ae33-9ccca56a342c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"00ca8bbe-ff35-4bb8-b8a4-0746a7563987\" class=\"plotly-graph-div\" style=\"height:400px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"00ca8bbe-ff35-4bb8-b8a4-0746a7563987\")) {                    Plotly.newPlot(                        \"00ca8bbe-ff35-4bb8-b8a4-0746a7563987\",                        [{\"mode\":\"lines\",\"name\":\"wdpa_2017\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[31.22604518134124,29.745476121169144,24.66867879757381,12.6247197238802,13.021626934805335,12.964184697883201,11.667998615732351,6.597145278207749,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"population_2015\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-0.04732181683002439,-0.04676274120328738,-0.044845675932679185,-0.038346133643952174,-0.015454765014037983,-0.007430669726452054,-0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"chirps_2017\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-0.13182888286202668,-0.1299543427932529,-0.12352621414086976,-0.10205019746261605,-0.02773878976308823,0.0,0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"maize\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[0.02319147179386618,0.022959326924187763,0.022163330843345975,0.020072700359757566,0.01713670792719678,0.01462828755676534,0.00182195598734811,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"soy\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-0.05697437784562185,-0.055489544912750495,-0.050398191207747765,-0.038390418610362666,-0.01509984495963584,-0.006219961956626813,-0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"sugarcane\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[0.017851690360472575,0.017390994883524472,0.01581131701927285,0.011938856103373434,0.002732469382362723,-0.0,-0.0,0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"perm_water\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[100.40374466131297,95.6855784474407,79.50702676276838,19.898823979676095,-0.0,-0.0,-0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"travel_min\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[0.032641901795068005,0.03259520301377447,0.032435074207208245,0.0319094848555387,0.03057618628383731,0.027311031701488936,0.020545937410058713,0.020641833484071313,0.010150134606621045,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"cropland\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[18.243985936055978,18.157740891909526,17.86202470427784,16.87957699301253,14.092242548306483,0.0,0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"mean_elev\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[0.059625301349111655,0.05853052728199408,0.05477654461458655,0.04237354837865487,0.005257212340653743,-0.0,-0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"sd_elev\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[0.8381434480170715,0.8385123484081882,0.8397771288568189,0.8403873374469294,0.8178804349391133,0.596923376363885,0.31114413484167147,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"near_road\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[0.22581167358048462,0.22570360964229033,0.22533324685029052,0.22298844350165484,0.21286096664725498,0.14051223729265144,0.0909675553846127,0.09897798104104456,0.015523316477161294,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"wdpa_2017_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-18.970690025326938,-17.461448889161737,-12.286334294235035,-0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"population_2015_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[1.2463518086221748e-05,1.2274425857375191e-05,1.162603219597242e-05,9.445865910440736e-06,1.6827445726406732e-06,-0.0,-0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"chirps_2017_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[4.435898123501435e-05,4.384290328497132e-05,4.2073176605427475e-05,3.617585805104308e-05,1.5797299360201146e-05,7.728463906379231e-06,4.956578307212974e-06,8.045981374820176e-07,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"maize_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-1.4702175561752806e-06,-1.4206120248398512e-06,-1.2505177291316166e-06,-8.004956137686329e-07,-0.0,1.2316950218496894e-07,8.579892900927296e-07,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"soy_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[8.457309597022986e-06,7.76591348071859e-06,5.395176964122377e-06,-0.0,-1.0055856099305566e-05,-1.1255829553780428e-05,-4.863913460434207e-06,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"sugarcane_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-3.4554392027967583e-06,-3.3719063124464663e-06,-3.0854815798268453e-06,-2.3962188821851908e-06,-8.714604041341549e-07,-4.309665534232034e-07,-0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"perm_water_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-53.935668202951874,-52.06579643814503,-45.65401775391543,-22.273940819771056,-12.792472839655751,-9.692874684998907,-1.9536742764598938,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"travel_min_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-7.759143617466074e-06,-7.731434561539513e-06,-7.636420572384354e-06,-7.316597190096211e-06,-6.3263525195745004e-06,-3.970148343787551e-06,-0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"cropland_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-27.632735277980625,-27.444700510944564,-26.799953429620786,-24.66768812471293,-18.230836569649913,-0.0,-0.0,-0.0,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"mean_elev_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-9.08319074358215e-05,-8.983258000875709e-05,-8.640586778977729e-05,-7.479176680264217e-05,-3.7608241901672816e-05,-3.204321019883024e-05,-3.747492718084616e-05,-2.0033061299531185e-05,-0.0,-0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"sd_elev_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-0.004992665246227682,-0.004993185283094426,-0.004994966368329031,-0.004955876271185986,-0.004601747645375817,-0.0023598280256238763,0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"near_road_sq\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[-0.000749111014764918,-0.0007463719583674044,-0.0007369803097338267,-0.0007053829616008235,-0.000598562665061479,-0.00026767066743959813,-0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"MSE Train\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[593.6572605769544,593.6735596871068,593.8629922886026,595.5392513528494,606.4459682099839,623.3689796969982,671.3460031379188,761.6873976917911,1033.6121082215604,1216.5256360173473],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"MSE Test\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[627.6880940547996,627.4619761484446,626.8464795060513,625.9840114636719,630.4432231741168,642.7205008794657,687.5827128951875,777.4747030754492,1037.5667880975038,1213.0058147482007],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"alpha\",\"x\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"y\":[1.52587890625e-05,5.232202043780965e-05,0.00017941094876411116,0.0006151958251439805,0.002109491677524035,0.0072333961897444584,0.024803141437003108,0.08504937501089847,0.29163225989402897,1.0],\"type\":\"scatter\"},{\"line\":{\"color\":\"black\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"bestAlpha\",\"x\":[0.0006151958251439805,0.0006151958251439805],\"y\":[-53.39631152092235,1228.6908923775209],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"type\":\"log\",\"autorange\":\"reversed\"},\"yaxis\":{\"range\":[-53.39631152092235,1228.6908923775209]},\"width\":800,\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('00ca8bbe-ff35-4bb8-b8a4-0746a7563987');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import plotly.graph_objects as go\n",
        "# Create traces\n",
        "fig = go.Figure()\n",
        "\n",
        "for strBeta in resCoef.columns[1:]:\n",
        "  # ax.plot(resCoef['alpha'],resPlot[strBeta],label=strBeta)\n",
        "  fig.add_trace(go.Scatter(x=resCoef['alpha'], y=resCoef[strBeta],\n",
        "                      mode='lines',\n",
        "                      name=strBeta))\n",
        "\n",
        "# Find best alpha\n",
        "alphaBest = resCoef.loc[resCoef['MSE Test']==resCoef['MSE Test'].min(),'alpha'][0]\n",
        "\n",
        "rangeYLow = resCoef.min().min()*0.99\n",
        "rangeYHigh = resCoef.max().max()*1.01\n",
        "fig.add_trace(go.Scatter(x=[alphaBest,alphaBest], y=[rangeYLow,rangeYHigh],\n",
        "                    mode='lines',\n",
        "                    name='bestAlpha',line=dict(color='black', dash='dash')))\n",
        "\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    xaxis_type=\"log\",\n",
        "    xaxis = dict(\n",
        "       autorange='reversed'\n",
        "    ),\n",
        "    yaxis = dict(\n",
        "       range=[rangeYLow,rangeYHigh],\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZm0tXJ8DLDM"
      },
      "source": [
        "# Day 1 Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQzN5tNDZua"
      },
      "source": [
        "Today's lab will have you run an OLS and LASSO regression using the deforestation data you saw in the introduction slides and in todays lecture (see code above). \n",
        "\n",
        "In the above specification we work with a rather restricted, preselected set of explnatory variables. Also we restricted the sample size artifically. Now try to run a model with a larger set of explanatory variables and using the full data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u7l5mTf1zRwh"
      },
      "outputs": [],
      "source": [
        "# If you haven't already, load the deforestation data following the code above.\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M9wzA0btgEL1"
      },
      "outputs": [],
      "source": [
        "# Define target (dependent) variable (% forest cover for 2018)\n",
        "strY = 'perc_treecover'\n",
        "\n",
        "# Define a list of features names (explantory variables)\n",
        "lstX = [\n",
        "  'bean',\n",
        " 'carrot',\n",
        " 'cassava',\n",
        " 'chickpea',\n",
        " 'citrus',\n",
        " 'coffee',\n",
        " 'groundnut',\n",
        " 'maize',\n",
        " 'soy',\n",
        " 'sugarcane',\n",
        " 'tomato',\n",
        " 'wheat',\n",
        " 'perm_water',\n",
        " 'travel_min',\n",
        " 'defor_2001',\n",
        " 'defor_2002',\n",
        " 'defor_2003',\n",
        " 'defor_2004',\n",
        " 'defor_2005',\n",
        " 'defor_2006',\n",
        " 'defor_2007',\n",
        " 'defor_2008',\n",
        " 'defor_2009',\n",
        " 'defor_2010',\n",
        " 'defor_2011',\n",
        " 'defor_2012',\n",
        " 'defor_2013',\n",
        " 'defor_2014',\n",
        " 'defor_2015',\n",
        " 'defor_2016',\n",
        " 'defor_2017',\n",
        " 'defor_2018',\n",
        " 'wdpa_1990',\n",
        " 'wdpa_1991',\n",
        " 'wdpa_1993',\n",
        " 'wdpa_1994',\n",
        " 'wdpa_1995',\n",
        " 'wdpa_1996',\n",
        " 'wdpa_1997',\n",
        " 'wdpa_1998',\n",
        " 'wdpa_1999',\n",
        " 'wdpa_2000',\n",
        " 'wdpa_2001',\n",
        " 'wdpa_2002',\n",
        " 'wdpa_2003',\n",
        " 'wdpa_2004',\n",
        " 'wdpa_2005',\n",
        " 'wdpa_2006',\n",
        " 'wdpa_2007',\n",
        " 'wdpa_2008',\n",
        " 'wdpa_2009',\n",
        " 'wdpa_2010',\n",
        " 'wdpa_2011',\n",
        " 'wdpa_2012',\n",
        " 'wdpa_2014',\n",
        " 'wdpa_2015',\n",
        " 'wdpa_2017',\n",
        " 'wdpa_2018',\n",
        " 'chirps_2001',\n",
        " 'chirps_2002',\n",
        " 'chirps_2003',\n",
        " 'chirps_2004',\n",
        " 'chirps_2005',\n",
        " 'chirps_2006',\n",
        " 'chirps_2007',\n",
        " 'chirps_2008',\n",
        " 'chirps_2009',\n",
        " 'chirps_2010',\n",
        " 'chirps_2011',\n",
        " 'chirps_2012',\n",
        " 'chirps_2013',\n",
        " 'chirps_2014',\n",
        " 'chirps_2015',\n",
        " 'chirps_2016',\n",
        " 'chirps_2017',\n",
        " 'chirps_2018',\n",
        " 'population_2000',\n",
        " 'population_2005',\n",
        " 'population_2010',\n",
        " 'population_2015',\n",
        " 'cropland',\n",
        " 'pasture',\n",
        " 'mean_elev',\n",
        " 'sd_elev',\n",
        " 'near_road',\n",
        " 'bean_lag_1st_order',\n",
        " 'carrot_lag_1st_order',\n",
        " 'cassava_lag_1st_order',\n",
        " 'chickpea_lag_1st_order',\n",
        " 'citrus_lag_1st_order',\n",
        " 'coffee_lag_1st_order',\n",
        " 'groundnut_lag_1st_order',\n",
        " 'maize_lag_1st_order',\n",
        " 'soy_lag_1st_order',\n",
        " 'sugarcane_lag_1st_order',\n",
        " 'tomato_lag_1st_order',\n",
        " 'wheat_lag_1st_order',\n",
        " 'perm_water_lag_1st_order',\n",
        " 'travel_min_lag_1st_order',\n",
        " 'defor_2001_lag_1st_order',\n",
        " 'defor_2002_lag_1st_order',\n",
        " 'defor_2003_lag_1st_order',\n",
        " 'defor_2004_lag_1st_order',\n",
        " 'defor_2005_lag_1st_order',\n",
        " 'defor_2006_lag_1st_order',\n",
        " 'defor_2007_lag_1st_order',\n",
        " 'defor_2008_lag_1st_order',\n",
        " 'defor_2009_lag_1st_order',\n",
        " 'defor_2010_lag_1st_order',\n",
        " 'defor_2011_lag_1st_order',\n",
        " 'defor_2012_lag_1st_order',\n",
        " 'defor_2013_lag_1st_order',\n",
        " 'defor_2014_lag_1st_order',\n",
        " 'defor_2015_lag_1st_order',\n",
        " 'defor_2016_lag_1st_order',\n",
        " 'defor_2017_lag_1st_order',\n",
        " 'defor_2018_lag_1st_order',\n",
        " 'wdpa_1990_lag_1st_order',\n",
        " 'wdpa_1991_lag_1st_order',\n",
        " 'wdpa_1993_lag_1st_order',\n",
        " 'wdpa_1994_lag_1st_order',\n",
        " 'wdpa_1995_lag_1st_order',\n",
        " 'wdpa_1996_lag_1st_order',\n",
        " 'wdpa_1997_lag_1st_order',\n",
        " 'wdpa_1998_lag_1st_order',\n",
        " 'wdpa_1999_lag_1st_order',\n",
        " 'wdpa_2000_lag_1st_order',\n",
        " 'wdpa_2001_lag_1st_order',\n",
        " 'wdpa_2002_lag_1st_order',\n",
        " 'wdpa_2003_lag_1st_order',\n",
        " 'wdpa_2004_lag_1st_order',\n",
        " 'wdpa_2005_lag_1st_order',\n",
        " 'wdpa_2006_lag_1st_order',\n",
        " 'wdpa_2007_lag_1st_order',\n",
        " 'wdpa_2008_lag_1st_order',\n",
        " 'wdpa_2009_lag_1st_order',\n",
        " 'wdpa_2010_lag_1st_order',\n",
        " 'wdpa_2011_lag_1st_order',\n",
        " 'wdpa_2012_lag_1st_order',\n",
        " 'wdpa_2014_lag_1st_order',\n",
        " 'wdpa_2015_lag_1st_order',\n",
        " 'wdpa_2017_lag_1st_order',\n",
        " 'wdpa_2018_lag_1st_order',\n",
        " 'chirps_2001_lag_1st_order',\n",
        " 'chirps_2002_lag_1st_order',\n",
        " 'chirps_2003_lag_1st_order',\n",
        " 'chirps_2004_lag_1st_order',\n",
        " 'chirps_2005_lag_1st_order',\n",
        " 'chirps_2006_lag_1st_order',\n",
        " 'chirps_2007_lag_1st_order',\n",
        " 'chirps_2008_lag_1st_order',\n",
        " 'chirps_2009_lag_1st_order',\n",
        " 'chirps_2010_lag_1st_order',\n",
        " 'chirps_2011_lag_1st_order',\n",
        " 'chirps_2012_lag_1st_order',\n",
        " 'chirps_2013_lag_1st_order',\n",
        " 'chirps_2014_lag_1st_order',\n",
        " 'chirps_2015_lag_1st_order',\n",
        " 'chirps_2016_lag_1st_order',\n",
        " 'chirps_2017_lag_1st_order',\n",
        " 'chirps_2018_lag_1st_order',\n",
        " 'population_2000_lag_1st_order',\n",
        " 'population_2005_lag_1st_order',\n",
        " 'population_2010_lag_1st_order',\n",
        " 'population_2015_lag_1st_order',\n",
        " 'cropland_lag_1st_order',\n",
        " 'pasture_lag_1st_order',\n",
        " 'mean_elev_lag_1st_order',\n",
        " 'sd_elev_lag_1st_order',\n",
        " 'near_road_lag_1st_order',\n",
        " ]\n",
        "\n",
        "# Get target variable and features\n",
        "Y_all = df[strY]\n",
        "X_all = df.loc[:,lstX]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mXKxlWM3DQ6D"
      },
      "outputs": [],
      "source": [
        "# split the data into a train and test set as above\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "l5QKVCXzgbxp"
      },
      "outputs": [],
      "source": [
        "# Run an OLS model to predict percent forest cover (using the 'train' part of the previous split for the training)\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tuYe3OOhgnh8"
      },
      "outputs": [],
      "source": [
        "# Generate the predicted values for forest cover in the train and test set\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oqzGowruhSHx"
      },
      "outputs": [],
      "source": [
        "# Produce measures of model fit (R2 and MSE) for train and test data\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WrFTs0h0EIey"
      },
      "outputs": [],
      "source": [
        "# plot predicted versus actual forest cover for test set\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYkF2bMzGbrA"
      },
      "source": [
        "Now run the same specification using a LASSO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RbAtwaimEVlj"
      },
      "outputs": [],
      "source": [
        "# generate a LASSO function from sklearn\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n",
        "modLasso = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "d8Pkscz1EqfO"
      },
      "outputs": [],
      "source": [
        "# Use the same variable specification as for you OLS model and fit it \n",
        "# to the training data\n",
        "# ==============\n",
        "# Your code here\n",
        "# ==============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3GJ98XMhjNLM"
      },
      "outputs": [],
      "source": [
        "# Create fitted values for train and test set and compare model fit (R2/MSE)\n",
        "# as you have done it for OLS\n",
        "# ==============\n",
        "# Your code here\n",
        "# =============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GITrlka73R1j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7d9ed9f0-8753-4368-be7f-175582da67c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a52b46626bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check how many coefficients are selected in Lasso (i.e. are greater 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of total coefficients in Lasso: '\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodLasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of selected coefficients in Lasso: '\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodLasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nList selected variables:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'coef_'"
          ]
        }
      ],
      "source": [
        "# Check how many coefficients are selected in Lasso (i.e. are greater 0)\n",
        "print('Number of total coefficients in Lasso: ' , modLasso.coef_.shape[0])\n",
        "print('Number of selected coefficients in Lasso: ' ,np.sum(modLasso.coef_>0))\n",
        "\n",
        "print('\\nList selected variables:\\n')\n",
        "list(pd.Series(lstX).loc[modLasso.coef_>0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZR-HB4KFLXb"
      },
      "source": [
        "Now change your alpha (lambda) and repeat.  \n",
        "\n",
        "What happens as you increase your alphs? As you decrease your alphs? Are the results more or less similar to your OLS results?\n",
        "\n",
        "Note: In this specific case it might be quite difficult to beat the simple OLS result in terms of MSE/R2 in the test set, because OLS is not really overfitting in this example.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBE-pVQGFUwE"
      },
      "source": [
        "(Optional) So far we are just choosing our training and test set randomly.  Is this valid if there is high spatial correlation?  (if you like) try splitting your data based on latitude and re-run the OLS and the LASSO.  Do you expect your results to be better or worse than using the random split?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uvIS2VDE0KS"
      },
      "outputs": [],
      "source": [
        "# set a cut-point in the latitude variable to split data into a train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRicA12RICPA"
      },
      "outputs": [],
      "source": [
        "# re-run OLS and LASSO models using this new train and test set\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FeLEUX37xA28"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "b2dd1c5c1941d22dfbdf86f16c96d8db5c09a3d6da608d7809bd79814f897e15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}